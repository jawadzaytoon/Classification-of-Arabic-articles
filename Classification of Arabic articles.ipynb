{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "049ea1f0",
   "metadata": {},
   "source": [
    "# arabic dataset classifiction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c27a1338",
   "metadata": {},
   "source": [
    "### importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aac4d6eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk import word_tokenize\n",
    "import pyarabic.araby as ar           \n",
    "import nltk                         \n",
    "import string        \n",
    "import os\n",
    "import re\n",
    "from nltk.corpus import stopwords            \n",
    "from nltk.stem.porter import PorterStemmer \n",
    "from nltk.stem import SnowballStemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10af0c9c",
   "metadata": {},
   "source": [
    "### data_read_and_remove(Null.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b412aab6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\9961013738.UPS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\9961013738.UPS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "stop_words = list(set(stopwords.words('arabic')))  \n",
    "\n",
    "df = pd.read_csv('arabic_dataset_classifiction.csv',chunksize=10000)\n",
    "\n",
    "data = pd.concat(df)\n",
    "\n",
    "clean=[]\n",
    "\n",
    "data.dropna(axis=0,inplace=True)       # Remove 'Null' values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "316d0f29",
   "metadata": {},
   "source": [
    "This code is using the Natural Language Toolkit (NLTK) library to download two packages: \n",
    "1. 'stopwords' - a collection of stop words (words that are commonly excluded from search queries) \n",
    "2. 'punkt' - a package of tokenizers for natural language processing\n",
    "\n",
    "It then creates a list of stop words in Arabic, taken from the NLTK library.\n",
    "\n",
    "It then reads a CSV file called 'arabic_dataset_classification.csv' in chunks of 10,000 rows each. \n",
    "\n",
    "The code then concatenates the chunks into one dataframe and drops any rows with missing values. \n",
    "\n",
    "Finally, it creates an empty list called 'clean' which will be used for further preprocessing of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2058e6ca",
   "metadata": {},
   "source": [
    "### stopword.list_read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "06adf5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = pd.read_csv('list.txt')\n",
    "\n",
    "t2 = pd.read_csv('arabicST.txt')\n",
    "\n",
    "t3 = pd.read_csv('list.tsv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c7ebe8f",
   "metadata": {},
   "source": [
    "This code reads in 3 separate files, with different formats. The first file is a list.txt, which is read in with the pd.read_csv() method. This creates a DataFrame (t1) with the data from the list.txt file. \n",
    "\n",
    "The second file is an arabicST.txt, which is also read in with the pd.read_csv() method. This creates another DataFrame (t2) with the data from the arabicST.txt file. \n",
    "\n",
    "The third file is a list.tsv, which is read in with the pd.read_csv() method. This creates a third DataFrame (t3) with the data from the list.tsv file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652136c6",
   "metadata": {},
   "source": [
    "### filtaring "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd02aa5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "arabic_punctuations = '''`÷×؛<>_()*&^%][ـ،/:\"؟.,'{}~¦+|!”…“–ـ'''\n",
    "\n",
    "english_punctuations = string.punctuation     # Get all the special characters.   \n",
    "\n",
    "punctuations_list = arabic_punctuations + english_punctuations  \n",
    "\n",
    "stemmer = nltk.ISRIStemmer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a37f75",
   "metadata": {},
   "source": [
    "This code creates two strings of punctuation marks, one for Arabic and one for English. It then combines the two strings into a single list of punctuation marks. Finally, it creates an instance of the ISRIStemmer from the Natural Language Toolkit library. This stemmer is used for Arabic text and is a part of the NLTK library."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a670001",
   "metadata": {},
   "source": [
    "### count the number of words we have befor cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e2491359",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9240130\n"
     ]
    }
   ],
   "source": [
    "unique = [j.split() for j in data['text']]\n",
    "\n",
    "unique = pd.DataFrame(unique)\n",
    "\n",
    "print(unique.nunique().sum()) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12acfbf",
   "metadata": {},
   "source": [
    "The code reads the data from the data frame and splits each row into a list. It stores the resulting list in a new data frame called unique. It then uses the nunique() function to count the number of unique items in the data frame and sums them up. The final result is the number of unique words in the data frame."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c159e8",
   "metadata": {},
   "source": [
    "### cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f33c4f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index,i in enumerate(data['text']):\n",
    "    \n",
    "    text = re.sub('[a-zA-Z0-9]',' ',i)\n",
    "    \n",
    "    text = text.split()\n",
    "    \n",
    "    text = [word for word in text if word not in list(t3)]\n",
    "    \n",
    "    text = [word for word in text if word not in t1]\n",
    "    \n",
    "    text = [word for word in text if word not in t2]\n",
    "    \n",
    "    text = [word for word in text if word not in punctuations_list]\n",
    "    \n",
    "    text = [stemmer.stem(word) for word in text if word not in stop_words]\n",
    "    \n",
    "    text = ' '.join(text)\n",
    "    \n",
    "    text = text.replace(\"آ\", \"ا\")\n",
    "    text = text.replace(\"إ\", \"ا\")\n",
    "    text = text.replace(\"أ\", \"ا\")\n",
    "    text = text.replace(\"ؤ\", \"و\")\n",
    "    text = text.replace(\"ئ\", \"ي\")\n",
    "    clean.append(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed0b7617",
   "metadata": {},
   "source": [
    "This code is preprocessing data from a data set. It is taking each element from the \"text\" field of the data set and performing a series of steps to clean it. The steps include replacing certain characters with others, removing words from a predefined list, stemming each word using a stemmer, and removing stop words and punctuation. Once the text is fully preprocessed, it is added to the list \"clean\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a234b3a0",
   "metadata": {},
   "source": [
    "### count the number of words we have after cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ad13bafa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2761834\n"
     ]
    }
   ],
   "source": [
    "unique = [j.split() for j in clean]\n",
    "\n",
    "unique = pd.DataFrame(unique) \n",
    "\n",
    "print(unique.nunique().sum()) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb1b4d10",
   "metadata": {},
   "source": [
    "The code reads the data from the data frame and splits each row into a list. It stores the resulting list in a new data frame called unique. It then uses the nunique() function to count the number of unique items in the data frame and sums them up. The final result is the number of unique words in the data frame."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "332397de",
   "metadata": {},
   "source": [
    "#### countvectorizer and divide into X and y "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "600dbb28",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer(max_features=500000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef42d92",
   "metadata": {},
   "source": [
    "This code imports the CountVectorizer class from the scikit-learn library. CountVectorizer is a class used for extracting features from text documents and converting them into numeric representation. The max_features parameter specifies the maximum number of features (words or phrases) to be included in the CountVectorizer object. In this case, the maximum number of features is set to 500000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9c86710d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = cv.fit_transform(clean)\n",
    "\n",
    "y = data.iloc[:,1].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37763860",
   "metadata": {},
   "source": [
    "This code is part of a machine learning program.  It is segmenting the data into two parts, X and y. The X variable is being set to the result of a \"fit_transform\" function which uses \"clean\" as an input. This function is part of the \"cv\" library, which stands for \"cross-validation\". This library is used to help in the training of a machine learning model. The y variable is being set to the second column of the \"data\" variable, which is assumed to be a dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf1e4b3",
   "metadata": {},
   "source": [
    "### classifiction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dc54881f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2866    79   160   232   128]\n",
      " [   83  3578   114   233    93]\n",
      " [  126   115  2665   511    84]\n",
      " [  216   245   498  4183   119]\n",
      " [   95    98    75   121 10481]]\n",
      "\\Accuracy score for DECISION TREE: 0.8740716229134495\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# DECISION TREE\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "classifier = DecisionTreeClassifier()\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.25,random_state=0)\n",
    "\n",
    "classifier.fit(X_train,y_train)\n",
    "\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "# =============================================================================\n",
    "# DECISION TREE - TEST\n",
    "# =============================================================================\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm = confusion_matrix(y_test,y_pred)\n",
    "\n",
    "print(cm)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print(f\"\\Accuracy score for DECISION TREE: {accuracy_score(y_test,y_pred)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a7d140d",
   "metadata": {},
   "source": [
    "This code is using the sklearn library to create a Decision Tree Classifier and use it to predict a target variable. The Decision Tree Classifier is first initialized, then the data is split into training and test sets. The classifier is then fit on the training data, and then used to make predictions on the test set. Finally, a confusion matrix and accuracy score are calculated to evaluate the accuracy of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "42dee887",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy score: 0.9474961394220163\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# SVR RBF \n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "classifier = SVC()\n",
    "\n",
    "classifier.fit(X_train,y_train)\n",
    "\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "# =============================================================================\n",
    "# SVR RBF - TEST\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "print(f\"\\nAccuracy score: {accuracy_score(y_test,y_pred)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e40b7c8d",
   "metadata": {},
   "source": [
    "This code is using the Scikit-Learn library to create a Support Vector Machine (SVM) classifier. It is creating an instance of the SVM classifier, fitting it to the X_train and y_train data, then predicting the labels of the X_test data set. Finally, it prints out the accuracy score for the SVM classifier on the X_test data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "02160b0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy score for SVR LINEAR: 0.9301419222001618\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# SVR LINEAR\n",
    "# =============================================================================\n",
    "\n",
    "classifier = SVC(kernel='linear')\n",
    "\n",
    "classifier.fit(X_train,y_train)\n",
    "\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "# =============================================================================\n",
    "# SVR LINEAR - TEST\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "print(f\"\\nAccuracy score for SVR LINEAR: {accuracy_score(y_test,y_pred)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e1ac3d2",
   "metadata": {},
   "source": [
    "This code is creating a Support Vector Machine (SVM) classifier with a linear kernel, which is a supervised machine learning algorithm used for classification. It is then fitting the training data (X_train) to its corresponding labels (y_train) and using it to predict the labels of the test data (X_test). Finally, it is printing out the accuracy score of the SVM classifier's predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f4f88567",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy score: 0.9367232884770939\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# RANDOM FOREST \n",
    "# =============================================================================\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "classifier = RandomForestClassifier(n_estimators=100)\n",
    "\n",
    "classifier.fit(X_train,y_train)\n",
    "\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "# =============================================================================\n",
    "# RANDOM FOREST - TEST \n",
    "# =============================================================================\n",
    "\n",
    "print(f\"\\nAccuracy score: {accuracy_score(y_test,y_pred)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4689a8d",
   "metadata": {},
   "source": [
    "This code is using the RandomForestClassifier() function from the sklearn.ensemble library to create a machine learning model. The n_estimators parameter controls the number of trees in the forest. The .fit() method fits the training data to the model. Then the .predict() method is used to make predictions on the test data. Finally, the accuracy_score function is used to measure the model's performance by comparing the predicted values with the actual values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a370175e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
